# -*- coding: utf-8 -*-
"""Projekt końcowy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gDKV0pJuuH2b1MhhUSDm8Ic3xqYbDWnB

# Preprocessing data

### Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import re
import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
import seaborn as sns
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import keras.utils
import tensorflow as tf
import requests
from time import sleep
import pandas as pd
import re
from matplotlib import pyplot as plt
from random import randint
import re
!pip3 install instaloader
import instaloader
from datetime import datetime
from itertools import dropwhile, takewhile
import csv
!pip install gradio
import gradio as gr
import pickle
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

# %matplotlib notebook

"""### Loading dataset"""

!unzip '/content/labeled_data.csv (3).zip' -d '/content/sample_data'

HSdataset = pd.read_csv('/content/sample_data/labeled_data.csv')
HSdataset.describe()

HSdataset[HSdataset['class']==0].sample(20)

"""### Dataset cleaning"""

HSdataset.rename(columns={'tweet':'text'}, inplace=True)
HSdataset = HSdataset.drop(['count', 'offensive_language', 'neither', 'hate_speech', 'Unnamed: 0'], axis=1)
HSdataset['class_new'] = HSdataset['class'].map({0: 0, 1: 0,2: 1})
HSdataset.sample(40)

"""### Normalization"""

stopwords.words('english')

re1 = re.compile(r"^@.*\s$")
re2 = re.compile(r"\S*https?:\S*")
re3 = re.compile(r"[^a-zA-Z\s]+")
re4 = re.compile(r"rt")
re5 = re.compile(r"\n")
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def clean_text(text):
#zmiana liter na małe
  text = str(text).lower()
#usunięcie nicku -> @...
  text = re1.sub('', text)
#usunięcie linku
  text = re2.sub('', text)
#usunięcie znaków specjalnych i cyfr
  text = re3.sub('', text)
#usunięcie 'rt'
  text = re4.sub('', text)
#usunięcie /n
  text = re5.sub('', text)
#usunięcie stopwords w tokenizowanym tekście
  text = [word for word in text.split(' ') if word not in stopwords.words('english')]
  text=" ".join(text)
#Steamming
  text = [stemmer.stem(word) for word in text.split(' ')]
  text=" ".join(text)
#lematyzacja
  text = [lemmatizer.lemmatize(word) for word in text.split(' ')]
  text=" ".join(text)
  return text

HSdataset['text'] = HSdataset['text'].apply(clean_text)

HSdataset.sample(20) #można znaleźć wyrazy najbliższe np bihday

"""### Visualization"""

sns.countplot(HSdataset['class_new'])

# class count
class_count_0, class_count_1 = HSdataset['class_new'].value_counts()

# Separate class
class_0 = HSdataset[HSdataset['class_new'] == 0]
class_1 = HSdataset[HSdataset['class_new'] == 1] 
# print the shape of the class
print('class 0:', class_0.shape)
print('class 1:', class_1.shape)

#Undersampling
class_0_under = class_0.sample(class_count_1)

HSdataset = pd.concat([class_0_under, class_1], axis=0)

print("total class of 1 and 0:",HSdataset['class_new'].value_counts())
# plot the count after under-sampeling
HSdataset['class_new'].value_counts().plot(kind='bar', title='count (target)')

print((HSdataset.duplicated()==True).value_counts())

from wordcloud import WordCloud
import random
HateData = HSdataset[HSdataset['class_new']==1]

visual_rev = WordCloud().generate(' '.join(HateData['text']))
plt.figure(figsize=(8,8))
plt.imshow(visual_rev, interpolation='bilinear')
plt.show()

"""# Basic modelling

### Train test split
"""

from sklearn.model_selection import train_test_split

X = HSdataset['text']
y = HSdataset['class_new']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)

"""### TF-IDF/Logistic regression/Random Forest"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf.fit(X_train)

#Bag of words
tfidf.get_feature_names()

tfidf.get_params()

X_train_tf = tfidf.transform(X_train)
X_test_tf = tfidf.transform(X_test)
X_train_tf.toarray()

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train_tf, y_train)
y_pred = lr.predict(X_test_tf)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
rfc.fit(X_train_tf, y_train)
y_pred = rfc.predict(X_test_tf)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

important_features = pd.Series(data=rfc.feature_importances_,index=tfidf.get_feature_names())
important_features.sort_values(ascending=False,inplace=True)
important_features.head(20)

"""## Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
clf = clf.fit(X_train_tf, y_train)

clf.score(X_train_tf, y_train)

clf.score(X_test_tf, y_test)

"""# Neural network

## Tokenization
"""

max_words = 10000 #Bierzemy pod uwagę 10000 najczęściej występujących słów w zbiorze
maxlen = 100 #skraca komentarz do 100 słów

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)

word_index = tokenizer.word_index
print('Znaleziono %s unikatowych tokenów.' % len(word_index))

data = pad_sequences(sequences, maxlen=maxlen)

labels = np.asarray(y)
print('Kształt tensora danych:', data.shape)
print('Kształt tensora etykiet:', labels.shape)
print(tokenizer)

#Train, test, val split of tokenized text
X_train_tkn, X_test_tkn, y_train_tkn, y_test_tkn = train_test_split(data, labels, test_size=0.3, random_state=0, stratify=y)
#X_test_tkn, X_val_tkn, y_test_tkn, y_val_tkn = train_test_split(X_test_tkn, y_test_tkn, test_size=0.3, random_state=0, stratify=y_test_tkn)

"""## Glove Embedding"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

embeddings_index = {}
f = open('glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Znaleziono %s wektorów słów.' % len(embeddings_index))

embedding_dim = 100

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if i < max_words:
        if embedding_vector is not None:
            # Słowa nieznalezione w osadzanym indeksie zostaną zastąpione zerami.
            embedding_matrix[i] = embedding_vector

"""## Neural network with Glove embedding"""

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, SpatialDropout1D, LSTM
from tensorflow.keras.optimizers import RMSprop

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(loss='binary_crossentropy',optimizer='adam' ,metrics=['accuracy']) #adam optimizer

model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False

model.fit(X_train_tkn, y_train_tkn,
                    epochs=5,
                    batch_size=32,
                    validation_data=(X_test_tkn, y_test_tkn))

X_train_tkn

acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']
loss = history1.history['loss']
val_loss = history1.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania', color='m')
plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji', color='c')
plt.title('Dokladnosc trenowania i walidacji')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Strata trenowania', color='m')
plt.plot(epochs, val_loss, 'b', label='Strata walidacji', color='c')
plt.title('Strata trenowania i walidacji')
plt.legend()

plt.show()

"""## Neural network with self embedding"""

model2 = Sequential()
model2.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model2.add(SpatialDropout1D(0.2))
model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model2.add(Dense(1, activation='sigmoid'))
model2.summary()
model2.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])

history2 = model2.fit(X_train_tkn, y_train_tkn,
                    epochs=10,
                    batch_size=32,
                    validation_data=(X_test_tkn, y_test_tkn))

acc = history2.history['accuracy']
val_acc = history2.history['val_accuracy']
loss = history2.history['loss']
val_loss = history2.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania', color='m')
plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji', color='c')
plt.title('Dokladnosc trenowania i walidacji')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Strata trenowania', color='m')
plt.plot(epochs, val_loss, 'b', label='Strata walidacji', color='c')
plt.title('Strata trenowania i walidacji')
plt.legend()

plt.show()

"""# Instagram data

# App
"""

def make_prediction(comment_text):
    tokenizer.fit_on_texts(comment_text)
    sequences = tokenizer.texts_to_sequences(comment_text)
    data = pad_sequences(sequences, maxlen=maxlen)
    with open("model.pkl", "rb") as f:
        model  = pickle.load(f)
        preds = model.predict([[data]])
    if preds > 0.5:
        return "This comment is hate speech/offensive"
    return "This is neutral comment"
    #print(preds)


#Create the input component for Gradio
comment_input = gr.Textbox(label = "Enter comment")

app = gr.Interface(fn = make_prediction, inputs=comment_input, outputs=gr.Label(num_top_classes=1))
app.launch(debug=True)

comm = ['What r u wearing look at that hair hahahhahahhahaha']

make_prediction(comm)

comm = ['Beyond gorgeous and I love this song by']

make_prediction(comm)